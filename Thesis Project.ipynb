{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages to be used\n",
    "import networkx as nx\n",
    "from networkx.algorithms import approximation\n",
    "from networkx.algorithms import community\n",
    "from networkx.algorithms import centrality\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import ndlib.models.epidemics as ep\n",
    "import ndlib.models.ModelConfig as mc\n",
    "import random\n",
    "# import scipy\n",
    "# import math\n",
    "# import sys\n",
    "import pandas as pd\n",
    "# from fractions import Fraction\n",
    "# import timeit\n",
    "# from time import sleep\n",
    "import igraph as ig\n",
    "import time\n",
    "import collections\n",
    "# import heapq as heap\n",
    "# from tqdm import tqdm \n",
    "import community\n",
    "import json\n",
    "from ndlib.viz.mpl.DiffusionTrend import DiffusionTrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets, outside of definitions\n",
    "\n",
    "G1 = nx.read_edgelist(\"Wiki-Vote.txt\", create_using=nx.Graph(), nodetype= int)\n",
    "# print(G1)\n",
    "G2 = nx.read_edgelist(\"lastfm_asia_edges.txt\", create_using=nx.Graph(), nodetype=int)\n",
    "# G3 = nx.read_edgelist(\"facebook_combined.txt\", create_using=nx.Graph(), nodetype=int)\n",
    "G3 = nx.read_edgelist(\"musae_facebook_edges.txt\", create_using=nx.Graph(), nodetype=int)\n",
    "G4 = nx.read_edgelist(\"musae_ENGB_edges.txt\", create_using=nx.Graph(), nodetype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average shortest path length of the lastfm_asia dataset is,\", nx.average_shortest_path_length(G1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_degree = sum(dict(nx.degree(G1)).values()) / len(G1)\n",
    "print(\"The average degree of the Wiki-Vote dataset is,\", sum(dict(nx.degree(G1)).values()) / len(G1))\n",
    "print(\"The average degree of the lastfm_asia dataset is,\",sum(dict(nx.degree(G2)).values()) / len(G2))\n",
    "print(\"The average degree of the facebook dataset is,\",sum(dict(nx.degree(G3)).values()) / len(G3))\n",
    "print(\"The average degree of the twitch dataset is,\",sum(dict(nx.degree(G4)).values()) / len(G4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The diameter of the Wiki-Vote dataset is, \",max([max(j.values()) for (i,j) in nx.shortest_path_length(G1)]))\n",
    "print(\"The diameter of the lastfm_asia dataset is, \",max([max(j.values()) for (i,j) in nx.shortest_path_length(G2)]))\n",
    "print(\"The diameter of the facebook dataset is, \",max([max(j.values()) for (i,j) in nx.shortest_path_length(G3)]))\n",
    "print(\"The diameter of the twitch dataset is, \", max([max(j.values()) for (i,j) in nx.shortest_path_length(G4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "filename = \"musae_ENGB_edges.csv\"\n",
    "with open(filename, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Create empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges to graph\n",
    "for line in lines:\n",
    "    parts = line.strip().split(\",\")\n",
    "    if len(parts) == 2:\n",
    "        src, dst = parts\n",
    "        G.add_edge(src, dst)\n",
    "\n",
    "# Print some information about the graph\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following can be done for any dataset that consists of files. First take the nodes and the attributes from the csv file, the name of\n",
    "# which ends with \"_target.csv\". Then covnert the file ending with \"_edges.csv\" to a file \"edges_txt\". From this file extract the edges and\n",
    "# add them to the created graph.\n",
    "\n",
    "# Load CSV target file\n",
    "filename = \"musae_ENGB_target.csv\"\n",
    "delimiter = \",\" # or another character that separates columns\n",
    "with open(filename, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Create graph\n",
    "G1 = nx.Graph()\n",
    "\n",
    "# Add nodes from file with attributes\n",
    "for line in lines:\n",
    "    fields = line.strip().split(delimiter)\n",
    "    node_id = fields[0]\n",
    "    attribute_value = fields[1]\n",
    "    G1.add_node(node_id, attribute=attribute_value)\n",
    "\n",
    "# Load TXT file with edge pairs\n",
    "edge_file = \"musae_ENGB_edges.txt\"\n",
    "edge_delimiter = \"\\t\" # or another character that separates columns\n",
    "with open(edge_file, \"r\") as f:\n",
    "    edge_lines = f.readlines()\n",
    "\n",
    "# Add edges to graph\n",
    "for line in edge_lines:\n",
    "    fields = line.strip().split(edge_delimiter)\n",
    "    source = fields[0]\n",
    "    target = fields[1]\n",
    "    G1.add_edge(source, target)\n",
    "    \n",
    "print(G1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of reading both a txt file for the edges and then applyign the attributes to the nodes by using the json file.\n",
    "\n",
    "G = nx.read_edgelist(\"musae_facebook_edges.txt\", create_using=nx.Graph())\n",
    "\n",
    "with open(\"musae_facebook_features.json\") as f:\n",
    "    data_facebook = json.load(f)\n",
    "print(data_facebook)\n",
    "\n",
    "nx.set_node_attributes(G, data_facebook, \"attributes\")\n",
    "G.nodes[\"0\"][\"attributes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Get a dictionary of node attributes keyed by node\n",
    "node_attrs = nx.get_node_attributes(G1, 'attribute')\n",
    "\n",
    "# Group nodes based on their attributes using defaultdict\n",
    "grouped_nodes = defaultdict(list)\n",
    "for node, attr in node_attrs.items():\n",
    "    grouped_nodes[attr].append(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the nodes into groups according ot their attributes and then print then groups.\n",
    "for attr, nodes in grouped_nodes.items():\n",
    "    print(f\"Nodes with attribute {attr}: {nodes}\")\n",
    "\n",
    "# Print the individual nodes to show their attributes\n",
    "# for node, data in G1.nodes.data():\n",
    "#     print(f\"Node {node} has attribute value {data['attribute']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr,nodes in grouped_nodes.items():\n",
    "    print(f\"The length of the group with attribute {attr} is {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centrality measure\n",
    "# dc = nx.degree_centrality(G1)\n",
    "# print(\"Done with degree centrality\")\n",
    "# bc = nx.betweenness_centrality(G1)\n",
    "# print(\"Done with betweenness centrality\")\n",
    "cc = nx.closeness_centrality(G1)\n",
    "print(\"Done with closeness centrality\")\n",
    "# pagerank = nx.pagerank(G1)\n",
    "# print(\"Done with pagerank\")\n",
    "# ec = nx.eigenvector_centrality(G1)\n",
    "# print(\"Done with eigen vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_dc = sorted(dc.items(), key=lambda item: item[1], reverse=True)\n",
    "# sort_bc = sorted(bc.items(), key=lambda item: item[1], reverse=True)\n",
    "sort_cc = sorted(cc.items(), key=lambda item: item[1], reverse=True)\n",
    "# sort_pc = sorted(pc.items(), key=lambda item: item[1], reverse=True)\n",
    "# sort_ec = sorted(ec.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_keys= [k for k,v in sort_cc]\n",
    "print(only_keys)\n",
    "\n",
    "# Take the top 50 nodes to be used as seeds later\n",
    "first_100 = only_keys[0:100]\n",
    "print(first_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original graph to be modified later, by removing nodes\n",
    "G1_copy = G1.copy()\n",
    "\n",
    "\n",
    "G1_copy.remove_nodes_from(first_100)\n",
    "print(G1_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total number of nodes in the graph\n",
    "num_nodes = len(G1.nodes())\n",
    "\n",
    "# Initialize a dictionary to store the number of seeds for each group\n",
    "num_seeds = {}\n",
    "\n",
    "# Calculate the total number of seed nodes as a fraction of the total number of nodes\n",
    "# frac_seeds = 0.1\n",
    "k = 100\n",
    "\n",
    "\n",
    "# Iterate over the groups and calculate the number of seed nodes for each group\n",
    "for attr, nodes in grouped_nodes.items():\n",
    "    num_group_nodes = len(nodes)\n",
    "    # num_group_seeds = int(num_group_nodes / num_nodes * frac_seeds)\n",
    "    num_group_seeds = int((num_group_nodes / num_nodes) * k)\n",
    "\n",
    "    num_seeds[attr] = num_group_seeds\n",
    "\n",
    "# Sort the nodes in each group based on their centrality measure\n",
    "for attr, nodes in grouped_nodes.items():\n",
    "    # nodes.sort(key=lambda x: dc[x], reverse=True)\n",
    "    # nodes.sort(key=lambda x: bc[x], reverse=True)\n",
    "    nodes.sort(key=lambda x: cc[x], reverse=True)\n",
    "    # nodes.sort(key=lambda x: pc[x], reverse=True)\n",
    "    # nodes.sort(key=lambda x: ec[x], reverse=True)\n",
    "\n",
    "# Initialize a list to store the seed nodes\n",
    "seeds = []\n",
    "\n",
    "# Iterate over the groups and add the top seed nodes to the list\n",
    "for attr, nodes in grouped_nodes.items():\n",
    "    num_group_seeds = num_seeds[attr]\n",
    "    seeds += nodes[:num_group_seeds]\n",
    "\n",
    "# Print the seed nodes\n",
    "print(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original graph to be modified later, by removing nodes\n",
    "G1_copy = G1.copy()\n",
    "\n",
    "\n",
    "G1_copy.remove_nodes_from(seeds)\n",
    "print(G1_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Get a dictionary of node attributes keyed by node\n",
    "node_attrs = nx.get_node_attributes(G1_copy, 'attribute')\n",
    "\n",
    "# Group nodes based on their attributes using defaultdict\n",
    "grouped_nodes = defaultdict(list)\n",
    "for node, attr in node_attrs.items():\n",
    "    grouped_nodes[attr].append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centrality measure\n",
    "dc_after = nx.degree_centrality(G1_copy)\n",
    "print(\"Done with degree centrality\")\n",
    "bc_after = nx.betweenness_centrality(G1_copy)\n",
    "print(\"Done with betweenness centrality\")\n",
    "cc_after = nx.closeness_centrality(G1_copy)\n",
    "print(\"Done with closeness centrality\")\n",
    "pc_after = nx.pagerank(G1_copy)\n",
    "print(\"Done with pagerank\")\n",
    "ec_after = nx.eigenvector_centrality(G1_copy)\n",
    "print(\"Done with eigen vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total number of nodes in the graph\n",
    "num_nodes_after = len(G1_copy.nodes())\n",
    "\n",
    "# Initialize a dictionary to store the number of seeds for each group\n",
    "num_seeds = {}\n",
    "\n",
    "# Calculate the total number of seed nodes as a fraction of the total number of nodes\n",
    "frac_seeds = 0.1\n",
    "k = 100\n",
    "\n",
    "\n",
    "# Iterate over the groups and calculate the number of seed nodes for each group\n",
    "for attr, nodes in grouped_nodes.items():\n",
    "    num_group_nodes = len(nodes)\n",
    "    # num_group_seeds = int(num_group_nodes / num_nodes * frac_seeds)\n",
    "    num_group_seeds = int((num_group_nodes / num_nodes_after) * k)\n",
    "\n",
    "    num_seeds[attr] = num_group_seeds\n",
    "\n",
    "# Sort the nodes in each group based on their centrality measure\n",
    "for attr, nodes in grouped_nodes.items():\n",
    "    # nodes.sort(key=lambda x: dc_after[x], reverse=True)\n",
    "    # nodes.sort(key=lambda x: bc_after[x], reverse=True)\n",
    "    # nodes.sort(key=lambda x: cc_after[x], reverse=True)\n",
    "    # nodes.sort(key=lambda x: pc_after[x], reverse=True)\n",
    "    nodes.sort(key=lambda x: ec_after[x], reverse=True)\n",
    "\n",
    "# Initialize a list to store the seed nodes\n",
    "seeds_after = []\n",
    "\n",
    "# Iterate over the groups and add the top seed nodes to the list\n",
    "for attr, nodes in grouped_nodes.items():\n",
    "    num_group_seeds = num_seeds[attr]\n",
    "    seeds_after += nodes[:num_group_seeds]\n",
    "\n",
    "# Print the seed nodes\n",
    "print(seeds_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in G2.nodes:\n",
    "    G2.nodes[node]['gender'] = 'male' if (node % 2 == 0) else 'female'\n",
    "\n",
    "for node, attrs in G2.nodes(data=True):\n",
    "    print(f\"Node {node}: gender {attrs['gender']}\")\n",
    "\n",
    "male_nodes = [node for node,attr in G2.nodes(data='gender') if attr == 'male']\n",
    "female_nodes = [n for n, attr in G2.nodes(data='gender') if attr == 'female']\n",
    "\n",
    "bc = nx.betweenness_centrality(G2)\n",
    "\n",
    "# calculate betweenness centralities for male and female nodes\n",
    "male_bc = {n: bc[n] for n in male_nodes}\n",
    "female_bc = {n: bc[n] for n in female_nodes}\n",
    "\n",
    "print(\"Male nodes betweenness centralities:\", male_bc)\n",
    "print(\"Female nodes betweenness centralities:\", female_bc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristics to block/remove nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Centrality measures\n",
    "\n",
    "# Degree centrality\n",
    "\n",
    "degree_centrality = nx.centrality.degree_centrality(G2)\n",
    "sort_degree_centrality = sorted(degree_centrality.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Done with degree centrality\")\n",
    "\n",
    "# Betweenness\n",
    "\n",
    "# betweenness = nx.betweenness_centrality(G1)\n",
    "# sort_betweenness = sorted(betweenness.items(), key=lambda item: item[1], reverse=True)\n",
    "# print(\"Done with betweenness centrality\")\n",
    "\n",
    "\n",
    "# # # # Closeness\n",
    "\n",
    "# closeness = nx.closeness_centrality(G1)\n",
    "# sort_closeness = sorted(closeness.items(), key=lambda item: item[1], reverse=True)\n",
    "# print(\"Done with closeness centrality\")\n",
    "\n",
    "\n",
    "# # # # Influence maximization\n",
    "# Pagerank\n",
    "\n",
    "# pagerank = nx.pagerank(G1)\n",
    "# sort_pagerank = sorted(pagerank.items(), key=lambda item: item[1], reverse=True)\n",
    "# print(\"Done with pagerank centrality\")\n",
    "\n",
    "# # Eigenvector\n",
    "\n",
    "# eigenvector = nx.eigenvector_centrality(G1)\n",
    "# sort_eigenvector = sorted(eigenvector.items(), key=lambda item: item[1], reverse=True)\n",
    "# print(\"Done with eigenvector centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To only get the keys from the sorted list\n",
    "\n",
    "only_keys = [k for k,v in sort_pagerank]\n",
    "print(only_keys)\n",
    "\n",
    "# Take the top 50 nodes to be used as seeds later\n",
    "first_100 = only_keys[0:1400]\n",
    "print(first_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weight to the edges of the graph. To be used as thresholds for the ICM\n",
    "\n",
    "degree_dict = dict(G2.degree())\n",
    "weights = []\n",
    "for u, v, data in G2.edges(data=True):\n",
    "    ratio = 1 / degree_dict[v]\n",
    "    data['weight'] = ratio\n",
    "    weights.append(ratio)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original graph to be modified later, by removing nodes\n",
    "G1_copy = G1.copy()\n",
    "\n",
    "\n",
    "G1_copy.remove_nodes_from(first_100)\n",
    "print(G1_copy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the centrality measures for the new graph (After removing the nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality measures\n",
    "\n",
    "# Degree centrality\n",
    "degree_centrality_after = nx.centrality.degree_centrality(G1_copy)\n",
    "sort_degree_centrality_after = sorted(degree_centrality_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Done with degree centrality\")\n",
    "\n",
    "# Betweenness\n",
    "\n",
    "betweenness_after = nx.betweenness_centrality(G1_copy)\n",
    "sort_betweenness_after = sorted(betweenness_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Done with betweenness centrality\")\n",
    "\n",
    "# Closeness\n",
    "\n",
    "closeness_after = nx.closeness_centrality(G1_copy)\n",
    "sort_closeness_after = sorted(closeness_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Done with closeness centrality\")\n",
    "\n",
    "\n",
    "# Influence maximization\n",
    "# Pagerank\n",
    "\n",
    "pagerank_after = nx.pagerank(G1_copy)\n",
    "sort_pagerank_after = sorted(pagerank_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Done with pagerank centrality\")\n",
    "\n",
    "# Eigenvector\n",
    "\n",
    "eigenvector_after = nx.eigenvector_centrality(G1_copy)\n",
    "sort_eigenvector_after = sorted(eigenvector_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Done with eigenvector centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To only get the keys from the sorted list\n",
    "\n",
    "only_keys_after = [k for k,v in sort_pagerank_after]\n",
    "print(only_keys_after)\n",
    "\n",
    "# Take the top 50 nodes to be used as seeds later\n",
    "first_100_after = only_keys_after[0:100]\n",
    "print(first_100_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding weight to the edges of the graph\n",
    "\n",
    "# for u,v in G1.edges():\n",
    "#     G1[u][v]['weight'] = 1/G1.in_degree(v)\n",
    "# G1.get_edge_data(30,1412)\n",
    "# G1.in_degree(1412)\n",
    "\n",
    "# for u,v in G2.edges():\n",
    "#     G2[u][v]['weight'] = 1/G2.degree(v)\n",
    "# G2.get_edge_data(0,5)\n",
    "\n",
    "degree_dict = dict(G2_copy.degree())\n",
    "weights = []\n",
    "for u, v, data in G2_copy.edges(data=True):\n",
    "    ratio = 1 / degree_dict[v]\n",
    "    data['weight'] = ratio\n",
    "    weights.append(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds an attribute to the nodes of the graph, for example degree centrality\n",
    "\n",
    "nx.set_node_attributes(G1, centrality, \"centrality\")\n",
    "# nx.set_node_attributes(G2, centrality, \"centrality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the graph by removing edges based on their weight attribute\n",
    "\n",
    "for u,v in list(G1.edges()):\n",
    "    if G1[u][v]['weight'] < 0.04:\n",
    "        G1.remove_edge(u,v)\n",
    "G1_new = G1.copy()\n",
    "\n",
    "# Remove nodes based on their degree\n",
    "\n",
    "for u in list(G1.nodes()):\n",
    "    if G1.degree(u)< 10:\n",
    "        G1.remove_node(u)\n",
    "\n",
    "# Remove nodes based on their degree centrality\n",
    "\n",
    "for u,v in centrality.items():\n",
    "    if v < 0.004:\n",
    "        G1.remove_node(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of sources and targets based on in/out degrees of nodes\n",
    "\n",
    "sources = []\n",
    "targets = []\n",
    "for u in G1.nodes():\n",
    "    if G1.in_degree(u) == 0:\n",
    "        sources.append(u)\n",
    "for v in G1.nodes():\n",
    "    if G1.out_degree(v) == 0:\n",
    "        targets.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the sources to smaller components (useful for seed selection)\n",
    "\n",
    "sub_sources = sources[0:50]\n",
    "print(sub_sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random(nodes, n):\n",
    "    ''' Select seeds randomly\n",
    "    Args:\n",
    "        nodes (list) [#node]: node list of the graph;\n",
    "        n (int): number of seeds;\n",
    "    Returns: \n",
    "        seeds: (list) [#seed]: selected seed nodes index;\n",
    "    '''\n",
    "    # random.sample(nodes, n)\n",
    "    # return nodes[:n]\n",
    "    # np.random.shuffle(nodes)\n",
    "    # return nodes[:n]\n",
    "    seeds = random.sample(nodes, n)\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mia(nodes, edges, n):\n",
    "    ''' select seeds by mia policy\n",
    "    args:\n",
    "        nodes (list) [#node]: node list of the graph;\n",
    "        edges (list of list) [#edge, 2]: edge list of the graph;\n",
    "        n (int): number of seeds;\n",
    "    returns: \n",
    "        seeds: (list) [#seed]: selected seed nodes index;\n",
    "    '''\n",
    "    out_connection = {}\n",
    "    in_connection = {}\n",
    "    centrality_score = {}\n",
    "    seeds = []\n",
    "    for edge in edges:\n",
    "        if edge[0] in out_connection:\n",
    "            out_connection[edge[0]].append(edge[1])\n",
    "        else:\n",
    "            out_connection[edge[0]] = [edge[1]]\n",
    "        if edge[1] in in_connection:\n",
    "            in_connection[edge[1]] += 1\n",
    "        else:\n",
    "            in_connection[edge[1]] = 1\n",
    "    for node in nodes:\n",
    "        centrality_score[node] = mia_centrality(node, out_connection, in_connection)\n",
    "    i = 0\n",
    "    for node, _ in sorted(centrality_score.items(), key=lambda item: item[1], reverse=True):\n",
    "        if i >= n:\n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "        seeds.append(node)\n",
    "    return seeds\n",
    "\n",
    "def mia_centrality(node, out_connection, in_connection):\n",
    "    ''' select seeds by mia centrality policy\n",
    "    '''\n",
    "    theta = 0.5\n",
    "    c_score = 0\n",
    "    q = 1  # threshold of influence\n",
    "    visited = set()\n",
    "    path_prob = 1\n",
    "    c_score = dfs(visited, out_connection, path_prob, in_connection, node, theta, q)\n",
    "    return c_score\n",
    "\n",
    "def dfs(visited, out_connection, path_prob, in_connection, node, theta, q):\n",
    "    if node not in visited:\n",
    "        visited.add(node)\n",
    "        if node in out_connection:\n",
    "            for neighbour in out_connection[node]:\n",
    "                path_prob *= q / in_connection[neighbour]\n",
    "                if path_prob >= theta:\n",
    "                    dfs(visited, out_connection, path_prob, in_connection, neighbour, theta, q)\n",
    "                    path_prob /= (q / in_connection[neighbour])\n",
    "                else:\n",
    "                    path_prob /= (q / in_connection[neighbour])\n",
    "    N_of_nodes = len(visited)\n",
    "    return N_of_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree(edges, n):\n",
    "    ''' Select seeds by degree policy\n",
    "    Args:\n",
    "        edges (list of list) [#edge, 2]: edge list of the graph;\n",
    "        n (int): number of seeds;\n",
    "    Returns: \n",
    "        seeds: (list) [#seed]: selected seed nodes index;\n",
    "    '''\n",
    "    degree = {}\n",
    "    seeds = []\n",
    "    for edge in edges:\n",
    "        if edge[0] in degree:\n",
    "            degree[edge[0]] += 1\n",
    "        else:\n",
    "            degree[edge[0]] = 1\n",
    "        if edge[1] in degree:\n",
    "            degree[edge[1]] += 1\n",
    "        else:\n",
    "            degree[edge[1]] = 1\n",
    "    seeds = list({k: v for k, v in sorted(degree.items(), key=lambda item: item[1], reverse=True)}.keys())[:n]\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_discount(edges, n):\n",
    "    ''' Select seeds by degree discount degree\n",
    "    Args:\n",
    "        edges (list of list) [#edge, 2]: edge list of the graph;\n",
    "        n (int): number of seeds;\n",
    "    Returns: \n",
    "        seeds: (list) [#seed]: selected seed nodes index;\n",
    "    '''\n",
    "    out_degree = {}\n",
    "    connection = {}\n",
    "    seeds = []\n",
    "    for edge in edges:\n",
    "        if edge[1] in connection:\n",
    "            connection[edge[1]].append(edge[0])\n",
    "        else:\n",
    "            connection[edge[1]] = [edge[0]]\n",
    "        if edge[0] in out_degree:\n",
    "            out_degree[edge[0]] += 1\n",
    "        else:\n",
    "            out_degree[edge[0]] = 1\n",
    "    while len(seeds) < n:\n",
    "        seed = sorted(out_degree.items(), key=lambda item: item[1], reverse=True)[0][0]\n",
    "        seeds.append(seed)\n",
    "        out_degree[seed] = -1\n",
    "        if seed in connection:\n",
    "            for node in connection[seed]:\n",
    "                if node in out_degree:\n",
    "                    out_degree[node] -= 1\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_neighbor_fix(edges, n):\n",
    "    ''' select seeds by degree neighbor fix policy\n",
    "    args:\n",
    "        edges (list of list) [#edge, 2]: edge list of the graph;\n",
    "        n (int): number of seeds;\n",
    "    returns: \n",
    "        seeds: (list) [#seed]: selected seed nodes index;\n",
    "    '''\n",
    "    out_degree = {}\n",
    "    centrality_score = {}\n",
    "    connection = {}\n",
    "    seeds = []\n",
    "    for edge in edges:\n",
    "        if edge[1] in connection:\n",
    "            connection[edge[1]].append(edge[0])\n",
    "        else:\n",
    "            connection[edge[1]] = [edge[0]]\n",
    "        if edge[0] in out_degree:\n",
    "            out_degree[edge[0]] += 1\n",
    "        else:\n",
    "            out_degree[edge[0]] = 1\n",
    "    centrality_score = out_degree.copy()\n",
    "    for edge in edges:\n",
    "        if edge[1] in out_degree:\n",
    "            centrality_score[edge[0]] += out_degree[edge[1]]\n",
    "    while len(seeds) < n:\n",
    "        seed = sorted(centrality_score.items(), key=lambda item: item[1], reverse=True)[0][0]\n",
    "        seeds.append(seed)\n",
    "        centrality_score[seed] = -1\n",
    "        if seed in connection:\n",
    "            for node in connection[seed]:\n",
    "                if node in out_degree:\n",
    "                    centrality_score[node] -= out_degree[seed]\n",
    "\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_neighbor(edges, n):\n",
    "    ''' select seeds by degree neighbor policy\n",
    "    args:\n",
    "        edges (list of list) [#edge, 2]: edge list of the graph;\n",
    "        n (int): number of seeds;\n",
    "    returns: \n",
    "        seeds: (list) [#seed]: selected seed nodes index;\n",
    "    '''\n",
    "    out_degree = {}\n",
    "    centrality_score = {}\n",
    "    seeds = []\n",
    "    for edge in edges:\n",
    "        if edge[0] in out_degree:\n",
    "            out_degree[edge[0]] += 1\n",
    "        else:\n",
    "            out_degree[edge[0]] = 1\n",
    "    centrality_score = out_degree.copy()\n",
    "    for edge in edges:\n",
    "        if edge[1] in out_degree:\n",
    "            centrality_score[edge[0]] += out_degree[edge[1]]\n",
    "    seeds = sorted(centrality_score.items(), key=lambda item: item[1], reverse=True)\n",
    "    return seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create communities based on the louvain method\n",
    "\n",
    "louvain = nx.algorithms.community.louvain_communities(G2)\n",
    "print(len(louvain))\n",
    "# print(louvain[131])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seed set based on the communities and a heuristic\n",
    "\n",
    "seeds_new = {}\n",
    "for community in louvain:\n",
    "    max_node = max(community, key=lambda node: degree_centrality[node])\n",
    "    seeds_new[max_node] = community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy = 'degree'\n",
    "\n",
    "# seeds_number = int(len(G1.nodes) * init_rate)\n",
    "# seeds = list({k: v for k, v in sorted(dict(degreenew).items(), key=lambda item: item[1], reverse=True)}.keys())[:seeds_number]\n",
    "policy = 'mia'\n",
    "\n",
    "init_rate = 0.02\n",
    "# nodes = G2_new.nodes\n",
    "# edges = G2_new.edges\n",
    "# # nodes = G_new.vs\n",
    "# # edges = G_new.es\n",
    "nodes = G2.nodes()\n",
    "edges = G2.edges()\n",
    "seeds_number = int(len(nodes) * init_rate)\n",
    "\n",
    "if policy == 'degree':\n",
    "    seeds = degree(edges, seeds_number)\n",
    "    # seeds = list({k: v for k, v in sorted(dict(new_degree).items(), key=lambda item: item[1], reverse=True)}.keys())[:seeds_number]\n",
    "if policy == 'random':\n",
    "    seeds = random(nodes, seeds_number)\n",
    "if policy == 'degree_discount':\n",
    "    seeds = degree_discount(edges, seeds_number)\n",
    "if policy == 'mia':\n",
    "   seeds = mia(nodes, edges, seeds_number) \n",
    "if policy == 'degree_neighbor_fix':\n",
    "    seeds = degree_neighbor_fix(edges, seeds_number)\n",
    "if policy == 'degree_neighbor':\n",
    "    seeds = degree_neighbor(edges, seeds_number)\n",
    "print(f'Number of seeds: {len(seeds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seeds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use louvain communities without fairness measure and different centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "# Partition into communities\n",
    "partition = community.best_partition(G2)\n",
    "for node in G2.nodes:\n",
    "    G2.nodes[node]['community'] = partition[node]\n",
    "print(len(partition))\n",
    "\n",
    "num_distinct_values = len(set(partition.values()))\n",
    "\n",
    "print(\"The dictionary has\", num_distinct_values, \"distinct values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centralities (can also be done from the cell above, have to check to decide), for fairness this cell can be used to removed the selected nodes\n",
    "\n",
    "seeds = set()\n",
    "for community_id in set(partition.values()):\n",
    "    community_nodes = [node for node in G2.nodes if G2.nodes[node]['community'] == community_id]\n",
    "    max_degree_node = max(community_nodes, key=lambda node: degree_centrality[node])\n",
    "    seeds.add(max_degree_node)\n",
    "print(\"Number of seeds:\", len(seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original graph to be modified later, by removing nodes\n",
    "G1_copy_comm = G1.copy()\n",
    "\n",
    "\n",
    "G1_copy_comm.remove_nodes_from(seeds)\n",
    "print(G1_copy_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrality measures\n",
    "\n",
    "# Degree\n",
    "# sort_degree = sorted(dict(G2.degree).items(), key=lambda item: item[1], reverse=True)\n",
    "# print(sort_degree)\n",
    "\n",
    "# Degree centrality\n",
    "degree_centrality_after = nx.centrality.degree_centrality(G1_copy_comm)\n",
    "sort_degree_centrality_after = sorted(degree_centrality_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Degree centrality done\")\n",
    "\n",
    "# Betweenness\n",
    "\n",
    "betweenness_after = nx.betweenness_centrality(G1_copy_comm)\n",
    "sort_betweenness_after = sorted(betweenness_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Betweenness centrality done\")\n",
    "\n",
    "# Closeness\n",
    "\n",
    "closeness_after = nx.closeness_centrality(G1_copy_comm)\n",
    "sort_closeness_after = sorted(closeness_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Closeness centrality done\")\n",
    "\n",
    "\n",
    "# Influence maximization\n",
    "# Pagerank\n",
    "\n",
    "pagerank_after = nx.pagerank(G1_copy_comm)\n",
    "sort_pagerank_after = sorted(pagerank_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Pagerank done\")\n",
    "\n",
    "# Eigenvector\n",
    "\n",
    "eigenvector_after = nx.eigenvector_centrality(G1_copy_comm)\n",
    "sort_eigenvector_after = sorted(eigenvector_after.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Eigenvector done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "# Partition into communities\n",
    "partition = community.best_partition(G1_copy_comm)\n",
    "for node in G1_copy_comm.nodes:\n",
    "    G1_copy_comm.nodes[node]['community'] = partition[node]\n",
    "print(len(partition))\n",
    "\n",
    "num_distinct_values = len(set(partition.values()))\n",
    "\n",
    "print(\"The dictionary has\", num_distinct_values, \"distinct values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centralities (can also be done from the cell above, have to check to decide)\n",
    "\n",
    "seeds_after = set()\n",
    "for community_id in set(partition.values()):\n",
    "    community_nodes = [node for node in G1_copy_comm.nodes if G1_copy_comm.nodes[node]['community'] == community_id]\n",
    "    max_degree_node = max(community_nodes, key=lambda node: pagerank_after[node])\n",
    "    seeds_after.add(max_degree_node)\n",
    "print(\"Number of seeds:\", len(seeds_after))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"correct\" implementation for the equality fairness is bellow (as implemented in Farnandi's paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "# Calculcate communities using louvain method\n",
    "partition_original = community.best_partition(G2)\n",
    "for node in G2.nodes:\n",
    "    G2.nodes[node]['community'] = partition_original[node]\n",
    "print(len(partition_original))\n",
    "\n",
    "num_distinct_values = len(set(partition_original.values()))\n",
    "\n",
    "print(\"The dictionary has\", num_distinct_values, \"distinct values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the population ratio of each community:\n",
    "\n",
    "community_sizes = {}\n",
    "for c in set(partition_original.values()):\n",
    "    nodes_in_community = [n for n in partition_original.keys() if partition_original[n] == c]\n",
    "    community_sizes[c] = len(nodes_in_community)\n",
    "\n",
    "# Calculate the centralities for each node in the graph\n",
    "\n",
    "# closeness_centrality = nx.closeness_centrality(G2)\n",
    "# betweenness_centrality = nx.betweenness_centrality(G2)\n",
    "# degree_centrality = nx.degree_centrality(G2)\n",
    "# eigen_centrality = nx.eigenvector_centrality(G2)\n",
    "pagerank_centrality = nx.pagerank(G2)\n",
    "\n",
    "# Create a dictionary to store the centralities for each community:\n",
    "\n",
    "# community_closeness = {}\n",
    "# community_betweenness = {}\n",
    "# community_degree = {}\n",
    "# community_eigen = {}\n",
    "community_pagerank = {}\n",
    "for node in G2.nodes():\n",
    "    community = partition_original[node]\n",
    "    # if community not in community_closeness:\n",
    "    #     community_closeness[community] = 0\n",
    "    # if community not in community_betweenness:\n",
    "    #     community_betweenness[community] = 0\n",
    "    # if community not in community_degree:\n",
    "    #     community_degree[community] = 0\n",
    "    # if community not in community_eigen:\n",
    "    #     community_eigen[community] = 0\n",
    "    if community not in community_pagerank:\n",
    "        community_pagerank[community] = 0\n",
    "\n",
    "    # community_closeness[community] += closeness_centrality[node]\n",
    "    # community_betweenness[community] += betweenness_centrality[node]\n",
    "    # community_degree[community] += degree_centrality[node]\n",
    "    # community_eigen[community] += eigen_centrality[node]\n",
    "    community_pagerank[community] += pagerank_centrality[node]\n",
    "\n",
    "\n",
    "# Calculate the total centrality for each community\n",
    "\n",
    "# for community in community_closeness:\n",
    "#     community_closeness[community] /= community_sizes[community]\n",
    "# for community in community_betweenness:\n",
    "#     community_betweenness[community] /= community_sizes[community]\n",
    "# for community in community_degree:\n",
    "#     community_degree[community] /= community_sizes[community]\n",
    "# for community in community_eigen:\n",
    "#     community_eigen[community] /= community_sizes[community]\n",
    "for community in community_pagerank:\n",
    "    community_pagerank[community] /= community_sizes[community]\n",
    "\n",
    "# Calculate the number of seeds for each community proportional to the population ratio using the centralities:\n",
    "seeds = {}\n",
    "for node in G2.nodes():\n",
    "    community = partition_original[node]\n",
    "    if community not in seeds:\n",
    "        seeds[community] = []\n",
    "        # if len(seeds[community]) < community_sizes[community] * community_closeness[community]:\n",
    "        #     seeds[community].append(node)\n",
    "        # if len(seeds[community]) < community_sizes[community] * community_betweenness[community]:\n",
    "        #     seeds[community].append(node)\n",
    "        # if len(seeds[community]) < community_sizes[community] * community_degree[community]:\n",
    "        #     seeds[community].append(node)\n",
    "        # if len(seeds[community]) < community_sizes[community] * community_eigen[community]:\n",
    "        #     seeds[community].append(node)\n",
    "        if len(seeds[community]) < community_sizes[community] * community_pagerank[community]:\n",
    "            seeds[community].append(node)\n",
    "\n",
    "print(\"Number of seeds:\", len(seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original graph to be modified later, by removing nodes (fair version)\n",
    "G2_copy_comm_fair = G2.copy()\n",
    "\n",
    "\n",
    "G2_copy_comm_fair.remove_nodes_from(seeds)\n",
    "print(G2_copy_comm_fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G2_copy_comm_fair.degree())\n",
    "weights = []\n",
    "for u, v, data in G2_copy_comm_fair.edges(data=True):\n",
    "    ratio = 1 / degree_dict[v]\n",
    "    data['weight'] = ratio\n",
    "    weights.append(ratio)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "# Calculcate communities using louvain method\n",
    "partition = community.best_partition(G2_copy_comm_fair)\n",
    "for node in G2_copy_comm_fair.nodes:\n",
    "    G2_copy_comm_fair.nodes[node]['community'] = partition[node]\n",
    "print(len(partition))\n",
    "\n",
    "num_distinct_values = len(set(partition.values()))\n",
    "\n",
    "print(\"The dictionary has\", num_distinct_values, \"distinct values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_sizes = {}\n",
    "for c in set(partition.values()):\n",
    "    nodes_in_community = [n for n in partition.keys() if partition[n] == c]\n",
    "    community_sizes[c] = len(nodes_in_community)\n",
    "\n",
    "# Calculate the centralities for each node in the graph\n",
    "\n",
    "closeness_centrality = nx.closeness_centrality(G2_copy_comm_fair)\n",
    "betweenness_centrality = nx.betweenness_centrality(G2_copy_comm_fair)\n",
    "degree_centrality = nx.degree_centrality(G2_copy_comm_fair)\n",
    "eigen_centrality = nx.eigenvector_centrality(G2_copy_comm_fair)\n",
    "pagerank_centrality = nx.pagerank(G2_copy_comm_fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the population ratio of each community:\n",
    "\n",
    "# community_sizes = {}\n",
    "# for c in set(partition.values()):\n",
    "#     nodes_in_community = [n for n in partition.keys() if partition[n] == c]\n",
    "#     community_sizes[c] = len(nodes_in_community)\n",
    "\n",
    "# # Calculate the centralities for each node in the graph\n",
    "\n",
    "# # closeness_centrality = nx.closeness_centrality(G2_copy_comm_fair)\n",
    "# # betweenness_centrality = nx.betweenness_centrality(G2_copy_comm_fair)\n",
    "# degree_centrality = nx.degree_centrality(G2_copy_comm_fair)\n",
    "# # eigen_centrality = nx.eigenvector_centrality(G2_copy_comm_fair)\n",
    "# # pagerank_centrality = nx.pagerank(G2_copy_comm_fair)\n",
    "\n",
    "# Create a dictionary to store the centralities for each community:\n",
    "\n",
    "community_closeness = {}\n",
    "community_betweenness = {}\n",
    "community_degree = {}\n",
    "community_eigen = {}\n",
    "community_pagerank = {}\n",
    "for node in G2_copy_comm_fair.nodes():\n",
    "    community = partition[node]\n",
    "    if community not in community_closeness:\n",
    "        community_closeness[community] = 0\n",
    "    # if community not in community_betweenness:\n",
    "    #     community_betweenness[community] = 0\n",
    "    # if community not in community_degree:\n",
    "    #     community_degree[community] = 0\n",
    "    # if community not in community_eigen:\n",
    "    #     community_eigen[community] = 0\n",
    "    # if community not in community_pagerank:\n",
    "    #     community_pagerank[community] = 0\n",
    "\n",
    "    community_closeness[community] += closeness_centrality[node]\n",
    "    # community_betweenness[community] += betweenness_centrality[node]\n",
    "    # community_degree[community] += degree_centrality[node]\n",
    "    # community_eigen[community] += eigen_centrality[node]\n",
    "    # community_pagerank[community] += pagerank_centrality[node]\n",
    "\n",
    "\n",
    "# Calculate the total centrality for each community\n",
    "\n",
    "for community in community_closeness:\n",
    "    community_closeness[community] /= community_sizes[community]\n",
    "# for community in community_betweenness:\n",
    "#     community_betweenness[community] /= community_sizes[community]\n",
    "# for community in community_degree:\n",
    "#     community_degree[community] /= community_sizes[community]\n",
    "# for community in community_eigen:\n",
    "#     community_eigen[community] /= community_sizes[community]\n",
    "# for community in community_pagerank:\n",
    "#     community_pagerank[community] /= community_sizes[community]\n",
    "\n",
    "# Calculate the number of seeds for each community proportional to the population ratio using the centralities:\n",
    "seeds = {}\n",
    "for node in G2_copy_comm_fair.nodes():\n",
    "    community = partition[node]\n",
    "    if community not in seeds:\n",
    "        seeds[community] = []\n",
    "        if len(seeds[community]) < community_sizes[community] * community_closeness[community]:\n",
    "            seeds[community].append(node)\n",
    "        # if len(seeds[community]) < community_sizes[community] * community_betweenness[community]:\n",
    "        #     seeds[community].append(node)\n",
    "        # if len(seeds[community]) < community_sizes[community] * community_degree[community]:\n",
    "        #     seeds[community].append(node)\n",
    "        # if len(seeds[community]) < community_sizes[community] * community_eigen[community]:\n",
    "        #     seeds[community].append(node)\n",
    "        # if len(seeds[community]) < community_sizes[community] * community_pagerank[community]:\n",
    "        #     seeds[community].append(node)\n",
    "\n",
    "print(\"Number of seeds:\", len(seeds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new correct implementation for fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "# Compute the Louvain communities\n",
    "partition = community.best_partition(G1)\n",
    "\n",
    "# Compute the size of each community\n",
    "community_sizes = np.bincount(list(partition.values()))\n",
    "\n",
    "# Compute the total number of nodes in the graph\n",
    "total_nodes = len(G1.nodes())\n",
    "\n",
    "# Compute the number of communities and print it\n",
    "num_distinct_values = len(set(partition.values()))\n",
    "\n",
    "print(\"The dictionary has\", num_distinct_values, \"distinct values.\")\n",
    "\n",
    "proportional_sizes = community_sizes / total_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the seed nodes\n",
    "seeds = {}\n",
    "k=100\n",
    "for community_id in set(partition.values()):\n",
    "    nodes_in_community = [node for node, comm in partition.items() if comm == community_id]\n",
    "\n",
    "    # Compute the number of seed nodes for the current community\n",
    "    num_seeds = int(round(proportional_sizes[community_id] * len(nodes_in_community)))      \n",
    "\n",
    "    # Sort the nodes in the current community by centrality measure\n",
    "    # nodes_sorted_by_dc = sorted(nodes_in_community, key=lambda node: degree_centrality[node], reverse=True)\n",
    "    # nodes_sorted_by_bc = sorted(nodes_in_community, key=lambda node: betweenness[node], reverse=True)\n",
    "    # nodes_sorted_by_cc = sorted(nodes_in_community, key=lambda node: closeness[node], reverse=True)\n",
    "    nodes_sorted_by_pc = sorted(nodes_in_community, key=lambda node: pagerank[node], reverse=True)\n",
    "    # nodes_sorted_by_ec = sorted(nodes_in_community, key=lambda node: eigenvector[node], reverse=True)\n",
    "    \n",
    "    # Add the top `num_seeds` nodes to the seed dictionary \n",
    "    # seeds.update({node: degree_centrality[node] for node in nodes_sorted_by_dc[:num_seeds]})\n",
    "    # seeds.update({node: betweenness[node] for node in nodes_sorted_by_bc[:num_seeds]})\n",
    "    # seeds.update({node: closeness[node] for node in nodes_sorted_by_cc[:num_seeds]})\n",
    "    seeds.update({node: pagerank[node] for node in nodes_sorted_by_pc[:num_seeds]})\n",
    "    # seeds.update({node: eigenvector[node] for node in nodes_sorted_by_ec[:num_seeds]})\n",
    "\n",
    "print(len(seeds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original graph to be modified later, by removing nodes (fair version)\n",
    "G1_copy_comm_fair = G1.copy()\n",
    "\n",
    "\n",
    "G1_copy_comm_fair.remove_nodes_from(seeds)\n",
    "print(G1_copy_comm_fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte the centralities for the new graph\n",
    "\n",
    "degree_after = nx.degree_centrality(G1_copy_comm_fair)\n",
    "closeness_after = nx.closeness_centrality(G1_copy_comm_fair)\n",
    "betweenness_after = nx.betweenness_centrality(G1_copy_comm_fair)\n",
    "degree_after = nx.degree_centrality(G1_copy_comm_fair)\n",
    "eigen_after = nx.eigenvector_centrality(G1_copy_comm_fair)\n",
    "pagerank_after = nx.pagerank(G1_copy_comm_fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "# Compute the Louvain communities\n",
    "partition_after = community.best_partition(G1_copy_comm_fair)\n",
    "\n",
    "# Compute the size of each community\n",
    "\n",
    "community_sizes_after = np.bincount(list(partition_after.values()))\n",
    "\n",
    "# Compute the total number of nodes in the graph\n",
    "total_nodes_after = len(G1_copy_comm_fair.nodes())\n",
    "\n",
    "# Compute the number of communities and print it\n",
    "num_distinct_values = len(set(partition_after.values()))\n",
    "\n",
    "print(\"The dictionary has\", num_distinct_values, \"distinct values.\")\n",
    "\n",
    "proportional_sizes_after = community_sizes_after / total_nodes_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(partition_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the seed nodes\n",
    "seeds_after = {}\n",
    "for community_id in set(partition_after.values()):\n",
    "    nodes_in_community_after = [node for node, comm in partition_after.items() if comm == community_id]\n",
    "\n",
    "    # Compute the number of seed nodes for the current community\n",
    "    num_seeds_after = int(round(proportional_sizes_after[community_id] * len(nodes_in_community_after)))   \n",
    "    num_group_seeds = int((num_group_nodes / total_nodes_after) * k)\n",
    "\n",
    "    # Sort the nodes in the current community by betweenness centrality\n",
    "    # nodes_sorted_by_dc_after = sorted(nodes_in_community_after, key=lambda node: degree_after[node], reverse=True)\n",
    "    # nodes_sorted_by_bc_after = sorted(nodes_in_community_after, key=lambda node: betweenness_after[node], reverse=True)\n",
    "    # nodes_sorted_by_cc_after = sorted(nodes_in_community_after, key=lambda node: closeness_after[node], reverse=True)\n",
    "    # nodes_sorted_by_pc_after = sorted(nodes_in_community_after, key=lambda node: pagerank_after[node], reverse=True)\n",
    "    nodes_sorted_by_ec_after = sorted(nodes_in_community_after, key=lambda node: eigen_after[node], reverse=True)\n",
    "    \n",
    "    # Add the top `num_seeds` nodes to the seed dictionary \n",
    "    # seeds_after.update({node: degree_after[node] for node in nodes_sorted_by_dc_after[:num_seeds_after]})\n",
    "    # seeds_after.update({node: betweenness_after[node] for node in nodes_sorted_by_bc_after[:num_seeds_after]})\n",
    "    # seeds_after.update({node: closeness_after[node] for node in nodes_sorted_by_cc_after[:num_seeds_after]})\n",
    "    # seeds_after.update({node: pagerank_after[node] for node in nodes_sorted_by_pc_after[:num_seeds_after]})\n",
    "    seeds_after.update({node: eigen_after[node] for node in nodes_sorted_by_ec_after[:num_seeds_after]})\n",
    "\n",
    "print(len(seeds_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seeds_after))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following code dont forget to change the graphs for before/after blocking the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICM\n",
    "# This code can be used with the previous two cell blocks to calculate the influence count. \n",
    "# It combines the method of calculating the spread using spreading model as defined in the NDlib package, witht the different heuristic approaches.\n",
    "average = []\n",
    "for index in range(7):\n",
    "    model = ep.IndependentCascadesModel(G1)\n",
    "    \n",
    "    # Model configuration\n",
    "    cfg = mc.Configuration()\n",
    "    \n",
    "    infected_nodes = seeds\n",
    "    cfg.add_model_initial_configuration(\"Infected\", infected_nodes)\n",
    "    model.set_initial_status(cfg)\n",
    "\n",
    "    # Setting the edge parameters\n",
    "    for e in G1.edges():\n",
    "        cfg.add_edge_configuration(\"threshold\", e, random.uniform(0,1))\n",
    "\n",
    "    list = []\n",
    "    # Simulation execution\n",
    "    for i in range(0,1000):\n",
    "        iterations = model.iteration_bunch(1)\n",
    "\n",
    "    # Get infected nodes\n",
    "        dict = iterations[0]\n",
    "        for i in dict:\n",
    "            k = (i,dict[i])\n",
    "            list.append(k)\n",
    "    list2 = []\n",
    "    for i in range(len(list)):\n",
    "        if list[i][0] == 'status':\n",
    "            list2.append(list[i][1])\n",
    "    list4 = []\n",
    "    for t in range(len(list2)):\n",
    "        for f in list2[t]:\n",
    "            if list2[t][f] == 1:\n",
    "                list4.append(f)\n",
    " \n",
    "    percentage = (len(list4)/len(G1.nodes()))*100\n",
    "    \n",
    "    average.append(percentage)\n",
    "print(\"The average percentage is: \", sum(average)/len(average), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTM\n",
    "# This code can be used with the previous two cell blocks to calculate the influence count. \n",
    "# It combines the method of calculating the spread using spreading model as defined in the NDlib package, witht the different heuristic approaches.\n",
    "average = []\n",
    "for index in range(7):\n",
    "    model = ep.ThresholdModel(G1)\n",
    "    cfg = mc.Configuration()\n",
    "    infected_nodes = seeds\n",
    "    cfg.add_model_initial_configuration(\"Infected\", infected_nodes)\n",
    "    model.set_initial_status(cfg)\n",
    "    threshold = 0.1\n",
    "    for n in G1.nodes():\n",
    "        cfg.add_node_configuration(\"threshold\", n, random.uniform(0,1))\n",
    "\n",
    "    list = []\n",
    "    for i in range(0,1000):\n",
    "        iterations = model.iteration_bunch(1)\n",
    "\n",
    "        dict = iterations[0]\n",
    "        for i in dict:\n",
    "            k = (i,dict[i])\n",
    "            list.append(k)\n",
    "    list2 = []\n",
    "    for i in range(len(list)):\n",
    "        if list[i][0] == 'status':\n",
    "            list2.append(list[i][1])\n",
    "    list4 = []\n",
    "    for t in range(len(list2)):\n",
    "        for f in list2[t]:\n",
    "            if list2[t][f] == 1:\n",
    "                list4.append(f)\n",
    "    percentage = (len(list4)/len(G1.nodes()))*100\n",
    "    average.append(percentage)\n",
    "print(\"The average percentage is: \", sum(average)/len(average), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICM\n",
    "# This code can be used with the previous two cell blocks to calculate the influence count. \n",
    "# It combines the method of calculating the spread using spreading model as defined in the NDlib package, witht the different heuristic approaches.\n",
    "average = []\n",
    "for index in range(7):\n",
    "    model = ep.IndependentCascadesModel(G2_copy)\n",
    "    # model = ep.SIRModel(G2)\n",
    "    # model = ep.SIModel(G2)\n",
    "    # model = ep.ThresholdModel(G2_copy_comm_fair)\n",
    "    cfg = mc.Configuration()\n",
    "    # cfg.add_model_parameter('beta', 0.01)\n",
    "    # cfg.add_model_parameter('gamma', 0.005)\n",
    "    # cfg.add_model_parameter('lambda', 0.005)\n",
    "    infected_nodes = first_100_after\n",
    "    cfg.add_model_initial_configuration(\"Infected\", infected_nodes)\n",
    "    # cfg.add_model_parameter(\"fraction_infected\", 0.05)\n",
    "    model.set_initial_status(cfg)\n",
    "    threshold = 0.1\n",
    "    for e in G2_copy.edges():\n",
    "        cfg.add_edge_configuration(\"threshold\", e, 0.1)\n",
    "    # for n in G2.nodes():\n",
    "    #     cfg.add_node_configuration(\"threshold\", n, threshold)\n",
    "\n",
    "    # iterations = model.iteration_bunch(500, True, True)\n",
    "    # trends = model.build_trends(iterations)\n",
    "    # viz = DiffusionTrend(model, trends)\n",
    "    # p = viz.plot(width=800, height=800)\n",
    "    # show(p)\n",
    "    list = []\n",
    "    for i in range(0,50):\n",
    "        iterations = model.iteration_bunch(200)\n",
    "\n",
    "        dict = iterations[0]\n",
    "        for i in dict:\n",
    "            k = (i,dict[i])\n",
    "            list.append(k)\n",
    "    list2 = []\n",
    "    for i in range(len(list)):\n",
    "        if list[i][0] == 'status':\n",
    "            list2.append(list[i][1])\n",
    "    list4 = []\n",
    "    for t in range(len(list2)):\n",
    "        for f in list2[t]:\n",
    "            if list2[t][f] == 1:\n",
    "                list4.append(f)\n",
    "    # print(\"================================\")\n",
    "    # print(f\"Results for iteration {index+1}:\")\n",
    "    # print(\"Total nodes infected: \", len(list4), \"out of\", len(G2.nodes()))\n",
    "    percentage = (len(list4)/len(G2_copy.nodes()))*100\n",
    "    # print(\"Percentage of total infected nodes with current seed set: \", percentage, \"%\")\n",
    "    # print(\"================================\")\n",
    "    average.append(percentage)\n",
    "print(\"The average percentage is: \", sum(average)/len(average), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
